{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f79c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3246e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f990d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     topic  sentiment    stance     gender  \\\n",
      "0         Importance of Human Intervantion  -0.042726  believer       male   \n",
      "1                            Global stance   0.429441  believer  undefined   \n",
      "2                         Weather Extremes   0.092446   neutral       male   \n",
      "3  Ideological Positions on Global Warming  -0.337010    denier       male   \n",
      "4                         Weather Extremes  -0.317469    denier       male   \n",
      "\n",
      "   temperature_avg  aggressiveness  \\\n",
      "0        -4.082745      aggressive   \n",
      "1        -0.132076  not aggressive   \n",
      "2        -2.324198  not aggressive   \n",
      "3        -4.810226  not aggressive   \n",
      "4        -4.862617  not aggressive   \n",
      "\n",
      "                                                text  \n",
      "0  on march 29, 2008 at 8 pm, make a statement ab...  \n",
      "1  City looks at green building standards: Hoping...  \n",
      "2  @thiskat @agray @payload Snow? In PDX? In Marc...  \n",
      "3  someone alert al gore - global warming isn't w...  \n",
      "4  It's snowing in Langford and sticking!  It's t...  \n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 0','created_at', 'id', 'Full_Address', 'is_english','City', 'Continent', 'Country', 'State','lng','lat'], axis=1, inplace=True)\n",
    "\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb9dd30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic  sentiment  stance  gender  temperature_avg  aggressiveness  \\\n",
      "0      4  -0.042726       0       1        -4.082745               0   \n",
      "1      1   0.429441       0       2        -0.132076               1   \n",
      "2      9   0.092446       2       1        -2.324198               1   \n",
      "3      2  -0.337010       1       1        -4.810226               1   \n",
      "4      9  -0.317469       1       1        -4.862617               1   \n",
      "\n",
      "                                                text  \n",
      "0  on march 29, 2008 at 8 pm, make a statement ab...  \n",
      "1  City looks at green building standards: Hoping...  \n",
      "2  @thiskat @agray @payload Snow? In PDX? In Marc...  \n",
      "3  someone alert al gore - global warming isn't w...  \n",
      "4  It's snowing in Langford and sticking!  It's t...  \n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['aggressiveness'] = le.fit_transform(df['aggressiveness'])\n",
    "df['stance'] = le.fit_transform(df['stance'])\n",
    "df['topic'] = le.fit_transform(df['topic'])\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a29d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      topic  sentiment  stance  gender  temperature_avg  aggressiveness   00  \\\n",
      "0         4  -0.042726       0       1        -4.082745               0  0.0   \n",
      "1         1   0.429441       0       2        -0.132076               1  0.0   \n",
      "2         9   0.092446       2       1        -2.324198               1  0.0   \n",
      "3         2  -0.337010       1       1        -4.810226               1  0.0   \n",
      "4         9  -0.317469       1       1        -4.862617               1  0.0   \n",
      "...     ...        ...     ...     ...              ...             ...  ...   \n",
      "6602      2   0.189970       0       1         3.320110               1  0.0   \n",
      "6603      4  -0.194894       0       1        -1.603402               0  0.0   \n",
      "6604      4   0.170877       2       0        -0.691734               1  0.0   \n",
      "6605      1   0.668163       0       1         3.228544               1  0.0   \n",
      "6606      9  -0.255897       2       1         1.991308               1  0.0   \n",
      "\n",
      "      000  005317  008132  ...  zme   zo  zod  zombies  zomerkindje  zones  \\\n",
      "0     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "1     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "2     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "3     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "4     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "...   ...     ...     ...  ...  ...  ...  ...      ...          ...    ...   \n",
      "6602  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6603  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6604  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6605  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6606  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "\n",
      "      zoonotic  zoos  zzz  zzzzzzzzz  \n",
      "0          0.0   0.0  0.0        0.0  \n",
      "1          0.0   0.0  0.0        0.0  \n",
      "2          0.0   0.0  0.0        0.0  \n",
      "3          0.0   0.0  0.0        0.0  \n",
      "4          0.0   0.0  0.0        0.0  \n",
      "...        ...   ...  ...        ...  \n",
      "6602       0.0   0.0  0.0        0.0  \n",
      "6603       0.0   0.0  0.0        0.0  \n",
      "6604       0.0   0.0  0.0        0.0  \n",
      "6605       0.0   0.0  0.0        0.0  \n",
      "6606       0.0   0.0  0.0        0.0  \n",
      "\n",
      "[6607 rows x 11137 columns]\n"
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(x.toarray(), columns=v.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF vectors with the original DataFrame\n",
    "df_concatenated = pd.concat([df.drop('text', axis=1), tfidf_df], axis=1)\n",
    "\n",
    "print(df_concatenated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a499702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      topic  sentiment  gender  temperature_avg  aggressiveness   00  000  \\\n",
      "0         4  -0.042726       1        -4.082745               0  0.0  0.0   \n",
      "1         1   0.429441       2        -0.132076               1  0.0  0.0   \n",
      "2         9   0.092446       1        -2.324198               1  0.0  0.0   \n",
      "3         2  -0.337010       1        -4.810226               1  0.0  0.0   \n",
      "4         9  -0.317469       1        -4.862617               1  0.0  0.0   \n",
      "...     ...        ...     ...              ...             ...  ...  ...   \n",
      "6602      2   0.189970       1         3.320110               1  0.0  0.0   \n",
      "6603      4  -0.194894       1        -1.603402               0  0.0  0.0   \n",
      "6604      4   0.170877       0        -0.691734               1  0.0  0.0   \n",
      "6605      1   0.668163       1         3.228544               1  0.0  0.0   \n",
      "6606      9  -0.255897       1         1.991308               1  0.0  0.0   \n",
      "\n",
      "      005317  008132   01  ...  zme   zo  zod  zombies  zomerkindje  zones  \\\n",
      "0        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "1        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "2        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "3        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "4        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "...      ...     ...  ...  ...  ...  ...  ...      ...          ...    ...   \n",
      "6602     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6603     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6604     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6605     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6606     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "\n",
      "      zoonotic  zoos  zzz  zzzzzzzzz  \n",
      "0          0.0   0.0  0.0        0.0  \n",
      "1          0.0   0.0  0.0        0.0  \n",
      "2          0.0   0.0  0.0        0.0  \n",
      "3          0.0   0.0  0.0        0.0  \n",
      "4          0.0   0.0  0.0        0.0  \n",
      "...        ...   ...  ...        ...  \n",
      "6602       0.0   0.0  0.0        0.0  \n",
      "6603       0.0   0.0  0.0        0.0  \n",
      "6604       0.0   0.0  0.0        0.0  \n",
      "6605       0.0   0.0  0.0        0.0  \n",
      "6606       0.0   0.0  0.0        0.0  \n",
      "\n",
      "[6607 rows x 11135 columns] 0       0\n",
      "1       0\n",
      "2       2\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "6602    0\n",
      "6603    0\n",
      "6604    2\n",
      "6605    0\n",
      "6606    2\n",
      "Name: stance, Length: 6607, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "X = df_concatenated.drop('stance', axis=1)\n",
    "y = df['stance']\n",
    "\n",
    "\n",
    "print(X,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e8fd0bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8199697428139183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82       554\n",
      "           1       1.00      0.56      0.72       119\n",
      "           2       0.79      0.87      0.83       649\n",
      "\n",
      "    accuracy                           0.82      1322\n",
      "   macro avg       0.88      0.75      0.79      1322\n",
      "weighted avg       0.83      0.82      0.82      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfae2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('testingSet_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9012c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched States: ['England' 'Michigan' 'Missouri' 'Illinois' 'Manitoba' 'Kansas' 'Ohio'\n",
      " 'Kentucky' 'Queensland' 'Tennessee' 'Lazio' 'Kedah' 'Western Australia'\n",
      " 'District of Columbia' 'Scotland' 'São Paulo' 'Not Found' 'Minnesota'\n",
      " 'Punjab' 'Wisconsin' 'Oklahoma' 'Cotopaxi' 'Tamil Nadu'\n",
      " \"Provence-Alpes-Côte d'Azur\" 'Victoria' 'Beijing'\n",
      " 'Region of Southern Denmark' 'South Dakota' 'Lombardy' 'New South Wales'\n",
      " 'Otago' 'Andalusia' 'Catalonia' 'Hawaii' 'West' 'Auckland' 'West Bengal'\n",
      " 'Iowa' 'Eastern Cape' 'Greater Accra Region' 'Casanare' 'North Holland'\n",
      " 'Osh Region' 'Kerala' 'Indiana' 'Ile-de-France' 'North Dakota'\n",
      " 'North Rhine – Westphalia' 'Wellington' 'Bavaria' 'Wales'\n",
      " 'South Australia' 'Arkansas' 'Boyacá' 'Attica' 'Dubai' 'Shanghai'\n",
      " 'Minas Gerais' 'Puerto Rico' 'Chiang Mai Province' 'North Brabant'\n",
      " 'Maharashtra' 'Denmark' 'Gujarat' 'South Holland' 'Delhi' 'Western Cape'\n",
      " 'Dhaka Division' 'Central Serbia' 'Masovian Voivodeship' 'Antioquia'\n",
      " 'Mato Grosso' 'Karnataka' 'Selangor' 'Nebraska' 'Telangana' 'Kiambu'\n",
      " 'Bagmati Province' 'Lower Saxony' 'Riyadh Region' 'Berlin' 'Guayas'\n",
      " 'Uttar Pradesh' 'Nairobi County' 'Haryana' 'Vayots Dzor Province'\n",
      " 'Northern Ireland' 'Western Province' 'Utrecht' 'Réunion'\n",
      " 'Baden-Württemberg' 'Hong Kong' 'Chon Buri Province' 'Rio de Janeiro'\n",
      " 'Madrid' 'Nizhny Novgorod Oblast' 'Tehran Province' 'Lima'\n",
      " 'South District' 'Sabah' 'Santa Catarina' 'Tasmania']\n",
      "merged_region\n",
      "Northern North America    452\n",
      "Other                     886\n",
      "Southern North America    676\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "region_mapping = {\n",
    "    # Northeast North America\n",
    "    'Maine': 'Northeast North America', 'New Hampshire': 'Northeast North America', \n",
    "    'Vermont': 'Northeast North America', 'Massachusetts': 'Northeast North America', \n",
    "    'Rhode Island': 'Northeast North America', 'Connecticut': 'Northeast North America', \n",
    "    'New York': 'Northeast North America', 'New Jersey': 'Northeast North America', \n",
    "    'Pennsylvania': 'Northeast North America', 'Quebec': 'Northeast North America', \n",
    "    'New Brunswick': 'Northeast North America', 'Nova Scotia': 'Northeast North America', \n",
    "    'Prince Edward Island': 'Northeast North America', 'Newfoundland and Labrador': 'Northeast North America',\n",
    "    'Ontario': 'Northeast North America', 'Tamaulipas': 'Northeast North America',\n",
    "    \n",
    "    # Southeast North America\n",
    "    'Maryland': 'Southeast North America', 'Delaware': 'Southeast North America', \n",
    "    'Virginia': 'Southeast North America', 'West Virginia': 'Southeast North America', \n",
    "    'North Carolina': 'Southeast North America', 'South Carolina': 'Southeast North America', \n",
    "    'Georgia': 'Southeast North America', 'Florida': 'Southeast North America', \n",
    "    'Alabama': 'Southeast North America', 'Mississippi': 'Southeast North America', \n",
    "    'Louisiana': 'Southeast North America', 'Texas': 'Southeast North America', \n",
    "    'Veracruz': 'Southeast North America', 'Yucatan': 'Southeast North America', \n",
    "    'Quintana Roo': 'Southeast North America', 'Campeche': 'Southeast North America',\n",
    "    \n",
    "    # Northwest North America\n",
    "    'Washington': 'Northwest North America', 'Oregon': 'Northwest North America', \n",
    "    'Idaho': 'Northwest North America', 'Montana': 'Northwest North America', \n",
    "    'Wyoming': 'Northwest North America', 'British Columbia': 'Northwest North America', \n",
    "    'Alberta': 'Northwest North America', 'Saskatchewan': 'Northwest North America', \n",
    "    'Yukon': 'Northwest North America', 'Northwest Territories': 'Northwest North America', \n",
    "    'Nunavut': 'Northwest North America', 'Alaska': 'Northwest North America',\n",
    "    \n",
    "    # Southwest North America\n",
    "    'California': 'Southwest North America', 'Nevada': 'Southwest North America', \n",
    "    'Utah': 'Southwest North America', 'Arizona': 'Southwest North America', \n",
    "    'New Mexico': 'Southwest North America', 'Colorado': 'Southwest North America', \n",
    "    'Baja California': 'Southwest North America', 'Baja California Sur': 'Southwest North America', \n",
    "    'Sonora': 'Southwest North America', 'Chihuahua': 'Southwest North America',\n",
    "    'Sinaloa': 'Southwest North America', 'Nayarit': 'Southwest North America',\n",
    "}\n",
    "\n",
    "df_2['region'] = df_2['State'].map(region_mapping) # Apply the mapping to create a new 'region' column\n",
    "\n",
    "\n",
    "unmatched_states = df_2[df_2['region'].isnull()]['State'].unique() # Check for any states not matched and handle missing 'region' values\n",
    "if len(unmatched_states) > 0:\n",
    "    print(\"Unmatched States:\", unmatched_states)\n",
    "    df_2['region'].fillna('Region Not Classified', inplace=True)\n",
    "\n",
    "def merge_regions(region): # Define a function to map old regions to new merged regions (North Vs South)\n",
    "\n",
    "    if region in ['Northeast North America', 'Northwest North America']:\n",
    "        return 'Northern North America'\n",
    "    elif region in ['Southeast North America', 'Southwest North America']:\n",
    "        return 'Southern North America'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_2['merged_region'] = df_2['region'].apply(merge_regions)\n",
    "\n",
    "merged_region_counts = df_2.groupby('merged_region').size()\n",
    "print(merged_region_counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ac27487",
   "metadata": {},
   "outputs": [],
   "source": [
    "northern_north_america_df = df_2[df_2['merged_region'] == 'Northern North America'].copy()\n",
    "southern_north_america_df = df_2[df_2['merged_region'] == 'Southern North America'].copy()\n",
    "europe_df = df_2[df_2['Continent'] == 'Europe'].copy()\n",
    "\n",
    "#print(northern_north_america_df.shape)\n",
    "\n",
    "northern_north_america_df.drop(['Unnamed: 0', 'created_at', 'id', 'Full_Address', 'is_english', 'City', 'Continent', 'Country', 'State', 'lng', 'lat','merged_region','region'], axis=1, inplace=True)\n",
    "southern_north_america_df.drop(['Unnamed: 0', 'created_at', 'id', 'Full_Address', 'is_english', 'City', 'Continent', 'Country', 'State', 'lng', 'lat','merged_region','region'], axis=1, inplace=True)\n",
    "europe_df.drop(['Unnamed: 0', 'created_at', 'id', 'Full_Address', 'is_english', 'City', 'Continent', 'Country', 'State', 'lng', 'lat','merged_region','region'], axis=1, inplace=True)\n",
    "\n",
    "#print(northern_north_america_df.head(5))\n",
    "#print(southern_north_america_df.head(5))\n",
    "#print(europe_df.head(5))\n",
    "\n",
    "# Define a function to preprocess each subset\n",
    "def preprocess_df(df):\n",
    "    # Initialize LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    df['gender'] = le.fit_transform(df['gender'])\n",
    "    df['aggressiveness'] = le.fit_transform(df['aggressiveness'])\n",
    "    df['stance'] = le.fit_transform(df['stance'])\n",
    "    df['topic'] = le.fit_transform(df['topic'])\n",
    "    \n",
    "    # Initialize TfidfVectorizer\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    # Transform text data into TF-IDF vectors\n",
    "    x = v.fit_transform(df['text'])\n",
    "    \n",
    "    # Create DataFrame from TF-IDF vectors\n",
    "    tfidf_df = pd.DataFrame(x.toarray(), columns=v.get_feature_names_out())\n",
    "    \n",
    "    # Concatenate TF-IDF vectors with the original DataFrame\n",
    "    df_concatenated = pd.concat([df.drop('text', axis=1), tfidf_df], axis=1)\n",
    "    \n",
    "    return df_concatenated\n",
    "\n",
    "northern_north_america_df_preprocessed = preprocess_df(northern_north_america_df)# Preprocess each subset\n",
    "#southern_north_america_df_preprocessed = preprocess_df(southern_north_america_df)\n",
    "\n",
    "#print(northern_north_america_df_preprocessed.shape)\n",
    "#print(southern_north_america_df_preprocessed.head())\n",
    "#print(europe_df_preprocessed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4910ae5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- 01xq\n- 05rft\n- 090225132239\n- 10neu2\n- 122\n- ...\nFeature names seen at fit time, yet now missing:\n- 00\n- 005317\n- 008132\n- 01iis\n- 01t08e\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(northern_north_america_df_preprocessed)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:579\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    516\u001b[0m ):\n\u001b[0;32m    517\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:506\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    502\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m     )\n\u001b[1;32m--> 506\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- 01xq\n- 05rft\n- 090225132239\n- 10neu2\n- 122\n- ...\nFeature names seen at fit time, yet now missing:\n- 00\n- 005317\n- 008132\n- 01iis\n- 01t08e\n- ...\n"
     ]
    }
   ],
   "source": [
    "#unable to pass testing subset due to missalignment of vectors from tfidf, lost on what to do moving forward. \n",
    "y_pred = classifier.predict(northern_north_america_df_preprocessed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51f9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
