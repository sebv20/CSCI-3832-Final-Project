{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f79c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3246e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f990d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     topic  sentiment    stance     gender  \\\n",
      "0         Importance of Human Intervantion  -0.042726  believer       male   \n",
      "1                            Global stance   0.429441  believer  undefined   \n",
      "2                         Weather Extremes   0.092446   neutral       male   \n",
      "3  Ideological Positions on Global Warming  -0.337010    denier       male   \n",
      "4                         Weather Extremes  -0.317469    denier       male   \n",
      "\n",
      "   temperature_avg  aggressiveness  \\\n",
      "0        -4.082745      aggressive   \n",
      "1        -0.132076  not aggressive   \n",
      "2        -2.324198  not aggressive   \n",
      "3        -4.810226  not aggressive   \n",
      "4        -4.862617  not aggressive   \n",
      "\n",
      "                                                text  \n",
      "0  on march 29, 2008 at 8 pm, make a statement ab...  \n",
      "1  City looks at green building standards: Hoping...  \n",
      "2  @thiskat @agray @payload Snow? In PDX? In Marc...  \n",
      "3  someone alert al gore - global warming isn't w...  \n",
      "4  It's snowing in Langford and sticking!  It's t...  \n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 0','created_at', 'id', 'Full_Address', 'is_english','City', 'Continent', 'Country', 'State','lng','lat'], axis=1, inplace=True)\n",
    "\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb9dd30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topic  sentiment  stance  gender  temperature_avg  aggressiveness  \\\n",
      "0      4  -0.042726       0       1        -4.082745               0   \n",
      "1      1   0.429441       0       2        -0.132076               1   \n",
      "2      9   0.092446       2       1        -2.324198               1   \n",
      "3      2  -0.337010       1       1        -4.810226               1   \n",
      "4      9  -0.317469       1       1        -4.862617               1   \n",
      "\n",
      "                                                text  \n",
      "0  on march 29, 2008 at 8 pm, make a statement ab...  \n",
      "1  City looks at green building standards: Hoping...  \n",
      "2  @thiskat @agray @payload Snow? In PDX? In Marc...  \n",
      "3  someone alert al gore - global warming isn't w...  \n",
      "4  It's snowing in Langford and sticking!  It's t...  \n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['aggressiveness'] = le.fit_transform(df['aggressiveness'])\n",
    "df['stance'] = le.fit_transform(df['stance'])\n",
    "df['topic'] = le.fit_transform(df['topic'])\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a29d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      topic  sentiment  stance  gender  temperature_avg  aggressiveness   00  \\\n",
      "0         4  -0.042726       0       1        -4.082745               0  0.0   \n",
      "1         1   0.429441       0       2        -0.132076               1  0.0   \n",
      "2         9   0.092446       2       1        -2.324198               1  0.0   \n",
      "3         2  -0.337010       1       1        -4.810226               1  0.0   \n",
      "4         9  -0.317469       1       1        -4.862617               1  0.0   \n",
      "...     ...        ...     ...     ...              ...             ...  ...   \n",
      "6602      2   0.189970       0       1         3.320110               1  0.0   \n",
      "6603      4  -0.194894       0       1        -1.603402               0  0.0   \n",
      "6604      4   0.170877       2       0        -0.691734               1  0.0   \n",
      "6605      1   0.668163       0       1         3.228544               1  0.0   \n",
      "6606      9  -0.255897       2       1         1.991308               1  0.0   \n",
      "\n",
      "      000  005317  008132  ...  zme   zo  zod  zombies  zomerkindje  zones  \\\n",
      "0     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "1     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "2     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "3     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "4     0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "...   ...     ...     ...  ...  ...  ...  ...      ...          ...    ...   \n",
      "6602  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6603  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6604  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6605  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6606  0.0     0.0     0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "\n",
      "      zoonotic  zoos  zzz  zzzzzzzzz  \n",
      "0          0.0   0.0  0.0        0.0  \n",
      "1          0.0   0.0  0.0        0.0  \n",
      "2          0.0   0.0  0.0        0.0  \n",
      "3          0.0   0.0  0.0        0.0  \n",
      "4          0.0   0.0  0.0        0.0  \n",
      "...        ...   ...  ...        ...  \n",
      "6602       0.0   0.0  0.0        0.0  \n",
      "6603       0.0   0.0  0.0        0.0  \n",
      "6604       0.0   0.0  0.0        0.0  \n",
      "6605       0.0   0.0  0.0        0.0  \n",
      "6606       0.0   0.0  0.0        0.0  \n",
      "\n",
      "[6607 rows x 11137 columns]\n"
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(x.toarray(), columns=v.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF vectors with the original DataFrame\n",
    "df_concatenated = pd.concat([df.drop('text', axis=1), tfidf_df], axis=1)\n",
    "\n",
    "print(df_concatenated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a499702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      topic  sentiment  gender  temperature_avg  aggressiveness   00  000  \\\n",
      "0         4  -0.042726       1        -4.082745               0  0.0  0.0   \n",
      "1         1   0.429441       2        -0.132076               1  0.0  0.0   \n",
      "2         9   0.092446       1        -2.324198               1  0.0  0.0   \n",
      "3         2  -0.337010       1        -4.810226               1  0.0  0.0   \n",
      "4         9  -0.317469       1        -4.862617               1  0.0  0.0   \n",
      "...     ...        ...     ...              ...             ...  ...  ...   \n",
      "6602      2   0.189970       1         3.320110               1  0.0  0.0   \n",
      "6603      4  -0.194894       1        -1.603402               0  0.0  0.0   \n",
      "6604      4   0.170877       0        -0.691734               1  0.0  0.0   \n",
      "6605      1   0.668163       1         3.228544               1  0.0  0.0   \n",
      "6606      9  -0.255897       1         1.991308               1  0.0  0.0   \n",
      "\n",
      "      005317  008132   01  ...  zme   zo  zod  zombies  zomerkindje  zones  \\\n",
      "0        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "1        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "2        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "3        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "4        0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "...      ...     ...  ...  ...  ...  ...  ...      ...          ...    ...   \n",
      "6602     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6603     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6604     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6605     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "6606     0.0     0.0  0.0  ...  0.0  0.0  0.0      0.0          0.0    0.0   \n",
      "\n",
      "      zoonotic  zoos  zzz  zzzzzzzzz  \n",
      "0          0.0   0.0  0.0        0.0  \n",
      "1          0.0   0.0  0.0        0.0  \n",
      "2          0.0   0.0  0.0        0.0  \n",
      "3          0.0   0.0  0.0        0.0  \n",
      "4          0.0   0.0  0.0        0.0  \n",
      "...        ...   ...  ...        ...  \n",
      "6602       0.0   0.0  0.0        0.0  \n",
      "6603       0.0   0.0  0.0        0.0  \n",
      "6604       0.0   0.0  0.0        0.0  \n",
      "6605       0.0   0.0  0.0        0.0  \n",
      "6606       0.0   0.0  0.0        0.0  \n",
      "\n",
      "[6607 rows x 11135 columns] 0       0\n",
      "1       0\n",
      "2       2\n",
      "3       1\n",
      "4       1\n",
      "       ..\n",
      "6602    0\n",
      "6603    0\n",
      "6604    2\n",
      "6605    0\n",
      "6606    2\n",
      "Name: stance, Length: 6607, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "X = df_concatenated.drop('stance', axis=1)\n",
    "y = df['stance']\n",
    "\n",
    "\n",
    "print(X,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e8fd0bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8199697428139183\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82       554\n",
      "           1       1.00      0.56      0.72       119\n",
      "           2       0.79      0.87      0.83       649\n",
      "\n",
      "    accuracy                           0.82      1322\n",
      "   macro avg       0.88      0.75      0.79      1322\n",
      "weighted avg       0.83      0.82      0.82      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit the RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfae2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('testingSet_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9012c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched States: ['England' 'Michigan' 'Missouri' 'Illinois' 'Manitoba' 'Kansas' 'Ohio'\n",
      " 'Kentucky' 'Queensland' 'Tennessee' 'Lazio' 'Kedah' 'Western Australia'\n",
      " 'District of Columbia' 'Scotland' 'São Paulo' 'Not Found' 'Minnesota'\n",
      " 'Punjab' 'Wisconsin' 'Oklahoma' 'Cotopaxi' 'Tamil Nadu'\n",
      " \"Provence-Alpes-Côte d'Azur\" 'Victoria' 'Beijing'\n",
      " 'Region of Southern Denmark' 'South Dakota' 'Lombardy' 'New South Wales'\n",
      " 'Otago' 'Andalusia' 'Catalonia' 'Hawaii' 'West' 'Auckland' 'West Bengal'\n",
      " 'Iowa' 'Eastern Cape' 'Greater Accra Region' 'Casanare' 'North Holland'\n",
      " 'Osh Region' 'Kerala' 'Indiana' 'Ile-de-France' 'North Dakota'\n",
      " 'North Rhine – Westphalia' 'Wellington' 'Bavaria' 'Wales'\n",
      " 'South Australia' 'Arkansas' 'Boyacá' 'Attica' 'Dubai' 'Shanghai'\n",
      " 'Minas Gerais' 'Puerto Rico' 'Chiang Mai Province' 'North Brabant'\n",
      " 'Maharashtra' 'Denmark' 'Gujarat' 'South Holland' 'Delhi' 'Western Cape'\n",
      " 'Dhaka Division' 'Central Serbia' 'Masovian Voivodeship' 'Antioquia'\n",
      " 'Mato Grosso' 'Karnataka' 'Selangor' 'Nebraska' 'Telangana' 'Kiambu'\n",
      " 'Bagmati Province' 'Lower Saxony' 'Riyadh Region' 'Berlin' 'Guayas'\n",
      " 'Uttar Pradesh' 'Nairobi County' 'Haryana' 'Vayots Dzor Province'\n",
      " 'Northern Ireland' 'Western Province' 'Utrecht' 'Réunion'\n",
      " 'Baden-Württemberg' 'Hong Kong' 'Chon Buri Province' 'Rio de Janeiro'\n",
      " 'Madrid' 'Nizhny Novgorod Oblast' 'Tehran Province' 'Lima'\n",
      " 'South District' 'Sabah' 'Santa Catarina' 'Tasmania']\n",
      "merged_region\n",
      "Northern North America    452\n",
      "Other                     886\n",
      "Southern North America    676\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "region_mapping = {\n",
    "    # Northeast North America\n",
    "    'Maine': 'Northeast North America', 'New Hampshire': 'Northeast North America', \n",
    "    'Vermont': 'Northeast North America', 'Massachusetts': 'Northeast North America', \n",
    "    'Rhode Island': 'Northeast North America', 'Connecticut': 'Northeast North America', \n",
    "    'New York': 'Northeast North America', 'New Jersey': 'Northeast North America', \n",
    "    'Pennsylvania': 'Northeast North America', 'Quebec': 'Northeast North America', \n",
    "    'New Brunswick': 'Northeast North America', 'Nova Scotia': 'Northeast North America', \n",
    "    'Prince Edward Island': 'Northeast North America', 'Newfoundland and Labrador': 'Northeast North America',\n",
    "    'Ontario': 'Northeast North America', 'Tamaulipas': 'Northeast North America',\n",
    "    \n",
    "    # Southeast North America\n",
    "    'Maryland': 'Southeast North America', 'Delaware': 'Southeast North America', \n",
    "    'Virginia': 'Southeast North America', 'West Virginia': 'Southeast North America', \n",
    "    'North Carolina': 'Southeast North America', 'South Carolina': 'Southeast North America', \n",
    "    'Georgia': 'Southeast North America', 'Florida': 'Southeast North America', \n",
    "    'Alabama': 'Southeast North America', 'Mississippi': 'Southeast North America', \n",
    "    'Louisiana': 'Southeast North America', 'Texas': 'Southeast North America', \n",
    "    'Veracruz': 'Southeast North America', 'Yucatan': 'Southeast North America', \n",
    "    'Quintana Roo': 'Southeast North America', 'Campeche': 'Southeast North America',\n",
    "    \n",
    "    # Northwest North America\n",
    "    'Washington': 'Northwest North America', 'Oregon': 'Northwest North America', \n",
    "    'Idaho': 'Northwest North America', 'Montana': 'Northwest North America', \n",
    "    'Wyoming': 'Northwest North America', 'British Columbia': 'Northwest North America', \n",
    "    'Alberta': 'Northwest North America', 'Saskatchewan': 'Northwest North America', \n",
    "    'Yukon': 'Northwest North America', 'Northwest Territories': 'Northwest North America', \n",
    "    'Nunavut': 'Northwest North America', 'Alaska': 'Northwest North America',\n",
    "    \n",
    "    # Southwest North America\n",
    "    'California': 'Southwest North America', 'Nevada': 'Southwest North America', \n",
    "    'Utah': 'Southwest North America', 'Arizona': 'Southwest North America', \n",
    "    'New Mexico': 'Southwest North America', 'Colorado': 'Southwest North America', \n",
    "    'Baja California': 'Southwest North America', 'Baja California Sur': 'Southwest North America', \n",
    "    'Sonora': 'Southwest North America', 'Chihuahua': 'Southwest North America',\n",
    "    'Sinaloa': 'Southwest North America', 'Nayarit': 'Southwest North America',\n",
    "}\n",
    "\n",
    "df_2['region'] = df_2['State'].map(region_mapping) # Apply the mapping to create a new 'region' column\n",
    "\n",
    "\n",
    "unmatched_states = df_2[df_2['region'].isnull()]['State'].unique() # Check for any states not matched and handle missing 'region' values\n",
    "if len(unmatched_states) > 0:\n",
    "    print(\"Unmatched States:\", unmatched_states)\n",
    "    df_2['region'].fillna('Region Not Classified', inplace=True)\n",
    "\n",
    "def merge_regions(region): # Define a function to map old regions to new merged regions (North Vs South)\n",
    "\n",
    "    if region in ['Northeast North America', 'Northwest North America']:\n",
    "        return 'Northern North America'\n",
    "    elif region in ['Southeast North America', 'Southwest North America']:\n",
    "        return 'Southern North America'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_2['merged_region'] = df_2['region'].apply(merge_regions)\n",
    "\n",
    "merged_region_counts = df_2.groupby('merged_region').size()\n",
    "print(merged_region_counts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ac27487",
   "metadata": {},
   "outputs": [],
   "source": [
    "northern_north_america_df = df_2[df_2['merged_region'] == 'Northern North America'].copy()\n",
    "southern_north_america_df = df_2[df_2['merged_region'] == 'Southern North America'].copy()\n",
    "europe_df = df_2[df_2['Continent'] == 'Europe'].copy()\n",
    "\n",
    "#print(northern_north_america_df.shape)\n",
    "\n",
    "northern_north_america_df.drop(['Unnamed: 0', 'created_at', 'id', 'Full_Address', 'is_english', 'City', 'Continent', 'Country', 'State', 'lng', 'lat','merged_region','region'], axis=1, inplace=True)\n",
    "southern_north_america_df.drop(['Unnamed: 0', 'created_at', 'id', 'Full_Address', 'is_english', 'City', 'Continent', 'Country', 'State', 'lng', 'lat','merged_region','region'], axis=1, inplace=True)\n",
    "europe_df.drop(['Unnamed: 0', 'created_at', 'id', 'Full_Address', 'is_english', 'City', 'Continent', 'Country', 'State', 'lng', 'lat','merged_region','region'], axis=1, inplace=True)\n",
    "\n",
    "#print(northern_north_america_df.head(5))\n",
    "#print(southern_north_america_df.head(5))\n",
    "#print(europe_df.head(5))\n",
    "\n",
    "# Define a function to preprocess each subset\n",
    "def preprocess_df(df):\n",
    "    # Initialize LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    df['gender'] = le.fit_transform(df['gender'])\n",
    "    df['aggressiveness'] = le.fit_transform(df['aggressiveness'])\n",
    "    df['stance'] = le.fit_transform(df['stance'])\n",
    "    df['topic'] = le.fit_transform(df['topic'])\n",
    "    \n",
    "    # Initialize TfidfVectorizer\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    # Transform text data into TF-IDF vectors\n",
    "    x = v.fit_transform(df['text'])\n",
    "    \n",
    "    # Create DataFrame from TF-IDF vectors\n",
    "    tfidf_df = pd.DataFrame(x.toarray(), columns=v.get_feature_names_out())\n",
    "    \n",
    "    # Concatenate TF-IDF vectors with the original DataFrame\n",
    "    df_concatenated = pd.concat([df.drop('text', axis=1), tfidf_df], axis=1)\n",
    "    \n",
    "    return df_concatenated\n",
    "\n",
    "northern_north_america_df_preprocessed = preprocess_df(northern_north_america_df)# Preprocess each subset\n",
    "#southern_north_america_df_preprocessed = preprocess_df(southern_north_america_df)\n",
    "\n",
    "#print(northern_north_america_df_preprocessed.shape)\n",
    "#print(southern_north_america_df_preprocessed.head())\n",
    "#print(europe_df_preprocessed.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
