{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baxte\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "# torch ML libraries\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting intial variables and constants\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# graph Designs\n",
    "sns.set_theme(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# random seed for reproducibilty\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# set GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in \n",
    "df = pd.read_csv('full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6607, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature_avg</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>text</th>\n",
       "      <th>Full_Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>is_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-03-28 14:15:51+00:00</td>\n",
       "      <td>778602262</td>\n",
       "      <td>-93.101503</td>\n",
       "      <td>44.950404</td>\n",
       "      <td>Importance of Human Intervantion</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>believer</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.082745</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>on march 29, 2008 at 8 pm, make a statement ab...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-03-28 15:36:30+00:00</td>\n",
       "      <td>778649364</td>\n",
       "      <td>-117.164720</td>\n",
       "      <td>32.715710</td>\n",
       "      <td>Global stance</td>\n",
       "      <td>0.429441</td>\n",
       "      <td>believer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>-0.132076</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>City looks at green building standards: Hoping...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2008-03-28 15:41:13+00:00</td>\n",
       "      <td>778652293</td>\n",
       "      <td>-122.355847</td>\n",
       "      <td>37.788497</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>0.092446</td>\n",
       "      <td>neutral</td>\n",
       "      <td>male</td>\n",
       "      <td>-2.324198</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>@thiskat @agray @payload Snow? In PDX? In Marc...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2008-03-28 15:56:42+00:00</td>\n",
       "      <td>778661751</td>\n",
       "      <td>-123.033121</td>\n",
       "      <td>44.939157</td>\n",
       "      <td>Ideological Positions on Global Warming</td>\n",
       "      <td>-0.337010</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.810226</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>someone alert al gore - global warming isn't w...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Salem</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2008-03-28 16:09:12+00:00</td>\n",
       "      <td>778669248</td>\n",
       "      <td>-123.364953</td>\n",
       "      <td>48.428318</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>-0.317469</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.862617</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>It's snowing in Langford and sticking!  It's t...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 created_at         id         lng        lat  \\\n",
       "0           0  2008-03-28 14:15:51+00:00  778602262  -93.101503  44.950404   \n",
       "1           1  2008-03-28 15:36:30+00:00  778649364 -117.164720  32.715710   \n",
       "2           2  2008-03-28 15:41:13+00:00  778652293 -122.355847  37.788497   \n",
       "3           3  2008-03-28 15:56:42+00:00  778661751 -123.033121  44.939157   \n",
       "4           4  2008-03-28 16:09:12+00:00  778669248 -123.364953  48.428318   \n",
       "\n",
       "                                     topic  sentiment    stance     gender  \\\n",
       "0         Importance of Human Intervantion  -0.042726  believer       male   \n",
       "1                            Global stance   0.429441  believer  undefined   \n",
       "2                         Weather Extremes   0.092446   neutral       male   \n",
       "3  Ideological Positions on Global Warming  -0.337010    denier       male   \n",
       "4                         Weather Extremes  -0.317469    denier       male   \n",
       "\n",
       "   temperature_avg  aggressiveness  \\\n",
       "0        -4.082745      aggressive   \n",
       "1        -0.132076  not aggressive   \n",
       "2        -2.324198  not aggressive   \n",
       "3        -4.810226  not aggressive   \n",
       "4        -4.862617  not aggressive   \n",
       "\n",
       "                                                text  \\\n",
       "0  on march 29, 2008 at 8 pm, make a statement ab...   \n",
       "1  City looks at green building standards: Hoping...   \n",
       "2  @thiskat @agray @payload Snow? In PDX? In Marc...   \n",
       "3  someone alert al gore - global warming isn't w...   \n",
       "4  It's snowing in Langford and sticking!  It's t...   \n",
       "\n",
       "                                        Full_Address           City  \\\n",
       "0  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...     Saint Paul   \n",
       "1  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...      San Diego   \n",
       "2  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...  San Francisco   \n",
       "3  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...          Salem   \n",
       "4  {'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...       Victoria   \n",
       "\n",
       "       Continent        Country             State is_english  \n",
       "0  North America  United States         Minnesota       True  \n",
       "1  North America  United States        California       True  \n",
       "2  North America  United States        California       True  \n",
       "3  North America  United States            Oregon      False  \n",
       "4  North America         Canada  British Columbia       True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at df\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 1014 Negative: 1021 Neutral: 4572\n"
     ]
    }
   ],
   "source": [
    "# looking at sentiments \n",
    "pos = (df['sentiment'] > 0.5).sum()\n",
    "neg = (df['sentiment'] < -0.5).sum()\n",
    "neutral = len(df) - pos - neg\n",
    "print(f'Positive: {pos} Negative: {neg} Neutral: {neutral}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic to create classes for sentiment\n",
    "sentiment  = df[\"sentiment\"]\n",
    "\n",
    "conditions = [sentiment >= 0.5,\n",
    "              sentiment <= -0.5,\n",
    "              (sentiment >-0.5) & (sentiment <0.5)]\n",
    "choices  = [2,0,1]\n",
    "\n",
    "# create a new column in the DF based on the conditions\n",
    "df[\"sentiment_score\"] = np.select(conditions, choices, \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>topic</th>\n",
       "      <th>stance</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature_avg</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>text</th>\n",
       "      <th>Full_Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>is_english</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-03-28 14:15:51+00:00</td>\n",
       "      <td>778602262</td>\n",
       "      <td>-93.101503</td>\n",
       "      <td>44.950404</td>\n",
       "      <td>Importance of Human Intervantion</td>\n",
       "      <td>believer</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.082745</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>on march 29, 2008 at 8 pm, make a statement ab...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-03-28 15:36:30+00:00</td>\n",
       "      <td>778649364</td>\n",
       "      <td>-117.164720</td>\n",
       "      <td>32.715710</td>\n",
       "      <td>Global stance</td>\n",
       "      <td>believer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>-0.132076</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>City looks at green building standards: Hoping...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-03-28 15:41:13+00:00</td>\n",
       "      <td>778652293</td>\n",
       "      <td>-122.355847</td>\n",
       "      <td>37.788497</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>neutral</td>\n",
       "      <td>male</td>\n",
       "      <td>-2.324198</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>@thiskat @agray @payload Snow? In PDX? In Marc...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-03-28 15:56:42+00:00</td>\n",
       "      <td>778661751</td>\n",
       "      <td>-123.033121</td>\n",
       "      <td>44.939157</td>\n",
       "      <td>Ideological Positions on Global Warming</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.810226</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>someone alert al gore - global warming isn't w...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Salem</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-03-28 16:09:12+00:00</td>\n",
       "      <td>778669248</td>\n",
       "      <td>-123.364953</td>\n",
       "      <td>48.428318</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.862617</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>It's snowing in Langford and sticking!  It's t...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at         id         lng        lat  \\\n",
       "0  2008-03-28 14:15:51+00:00  778602262  -93.101503  44.950404   \n",
       "1  2008-03-28 15:36:30+00:00  778649364 -117.164720  32.715710   \n",
       "2  2008-03-28 15:41:13+00:00  778652293 -122.355847  37.788497   \n",
       "3  2008-03-28 15:56:42+00:00  778661751 -123.033121  44.939157   \n",
       "4  2008-03-28 16:09:12+00:00  778669248 -123.364953  48.428318   \n",
       "\n",
       "                                     topic    stance     gender  \\\n",
       "0         Importance of Human Intervantion  believer       male   \n",
       "1                            Global stance  believer  undefined   \n",
       "2                         Weather Extremes   neutral       male   \n",
       "3  Ideological Positions on Global Warming    denier       male   \n",
       "4                         Weather Extremes    denier       male   \n",
       "\n",
       "   temperature_avg  aggressiveness  \\\n",
       "0        -4.082745      aggressive   \n",
       "1        -0.132076  not aggressive   \n",
       "2        -2.324198  not aggressive   \n",
       "3        -4.810226  not aggressive   \n",
       "4        -4.862617  not aggressive   \n",
       "\n",
       "                                                text  \\\n",
       "0  on march 29, 2008 at 8 pm, make a statement ab...   \n",
       "1  City looks at green building standards: Hoping...   \n",
       "2  @thiskat @agray @payload Snow? In PDX? In Marc...   \n",
       "3  someone alert al gore - global warming isn't w...   \n",
       "4  It's snowing in Langford and sticking!  It's t...   \n",
       "\n",
       "                                        Full_Address           City  \\\n",
       "0  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...     Saint Paul   \n",
       "1  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...      San Diego   \n",
       "2  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...  San Francisco   \n",
       "3  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...          Salem   \n",
       "4  {'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...       Victoria   \n",
       "\n",
       "       Continent        Country             State is_english  sentiment_score  \n",
       "0  North America  United States         Minnesota       True                1  \n",
       "1  North America  United States        California       True                1  \n",
       "2  North America  United States        California       True                1  \n",
       "3  North America  United States            Oregon      False                1  \n",
       "4  North America         Canada  British Columbia       True                1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking it out\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df = df.drop('sentiment', axis=1)\n",
    "df['sentiment_score'] = df['sentiment_score'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ded5d7a3144d8d82e8d9334e091555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb3bfa9bfd84227b1b1a847aea97fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9ae7e645eb4bf98fab48f7f2f61933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocessing \n",
    "\n",
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the common BERT tokens\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows we’re doing classification\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store length of each review \n",
    "token_lens = []\n",
    "\n",
    "# iterate through the content slide\n",
    "for txt in df.text:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of review lengths \n",
    "sns.displot(token_lens)\n",
    "plt.xlim([0, 80]);\n",
    "plt.xlabel('Token count')\n",
    "plt.title('Review Token Count Graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "    # constructor function \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # length  method\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    # get item  method\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        # encoded format to be returned \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% train data and 10% test and 10% validation data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader to release data in batches\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.text.to_numpy(),\n",
    "        targets=df.sentiment_score.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train, test and val data loaders\n",
    "MAX_LEN = 160\n",
    "BATCH_SIZE = 32\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples \n",
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT sentiment classification \n",
    "\n",
    "# loading the basic BERT model \n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the sentiment classifier class \n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    # constructor class \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    # forward propagation class\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "          return_dict=False\n",
    "        )\n",
    "        #  add a dropout layer \n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model and move to classifier\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of hidden units\n",
    "print(bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of iterations \n",
    "EPOCHS = 2\n",
    "\n",
    "# adam as our optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# set the loss function \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for a single training iteration\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())   \n",
    "            \n",
    "        # backward prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate performance\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# training loop \n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # show details \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "\n",
    "    # get model performance (accuracy and loss)\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    \n",
    "    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # if we beat previous performance\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
