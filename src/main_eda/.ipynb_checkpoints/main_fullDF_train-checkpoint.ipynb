{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change Sentiment Analysis\n",
    "\n",
    "#### Sebastian Vargas, Baxter Romero, Johnny Wilcox, Trajan Pei, Caleb Bettcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA:\n",
    "\n",
    "Hydrate Data Set $\\newline$\n",
    "Drop any missing long/lattitude rows $\\newline$\n",
    "Create global subset of tweets (COL: Timestamp, Tweet, Sentiment Score)$\\newline$\n",
    "Create subsets of global subset for individual countries$\\newline$\n",
    "Create and evaluate subsets of different countries\n",
    "\n",
    "Questions we should try to answer with EDA:\n",
    "\n",
    "Describe details about the dataset you’ve chosen – $\\newline$\n",
    "How many examples(size after analysis), $\\newline$\n",
    "What statistics do you observe about the examples (Both columns, and data shape)$\\newline$\n",
    "How did you create training/validation/test sets? $\\newline$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text \n",
    "\n",
    "import keras\n",
    "import keras.utils\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.optimizers\n",
    "from keras import utils as np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirst_Clean.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile, \\\n\u001b[0;32m     11\u001b[0m      \u001b[38;5;28mopen\u001b[39m(output_file_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[0;32m     13\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(infile)\n\u001b[0;32m     14\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(outfile)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset.csv'"
     ]
    }
   ],
   "source": [
    "# NOTE\n",
    "# in order to run this project drop in the .csv data set (called: \"The Climate Change Twitter Dataset\") \n",
    "# download at:\n",
    "# https://www.kaggle.com/datasets/deffro/the-climate-change-twitter-dataset\n",
    "\n",
    "#Script removes all non hydrated tweets from scraper\n",
    "\n",
    "input_file_path = 'Dataset.csv'\n",
    "output_file_path = 'First_Clean.csv'\n",
    "with open(input_file_path, mode='r', newline='', encoding='ISO-8859-1') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    header = next(reader)  \n",
    "    writer.writerow(header)  \n",
    "\n",
    "    for row in reader:\n",
    "        if len(row) == 11 and row[10]: \n",
    "            writer.writerow(row)\n",
    "            \n",
    "raw_data = pd.read_csv('First_Clean.csv')\n",
    "\n",
    "#remove null values (missed/blocked scrapes)\n",
    "raw_data['text'] = raw_data['text'].str.strip() \n",
    "raw_data.dropna(subset=['text'], inplace=True)\n",
    "null_counts = raw_data.isnull().sum()\n",
    "#raw_data.shape\n",
    "#print(null_counts)\n",
    "\n",
    "\n",
    "print(raw_data.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15789411\n",
      "5307538\n"
     ]
    }
   ],
   "source": [
    "# checking length\n",
    "print(len(tweets_df))\n",
    "\n",
    "# removing NAN values \n",
    "tweets_df = tweets_df.dropna()\n",
    "\n",
    "# checking length again\n",
    "print(len(tweets_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tweepy config \n",
    "\n",
    "# import tweepy\n",
    "# import os \n",
    "# from dotenv import load_dotenv   \n",
    "# load_dotenv()                    \n",
    "\n",
    "# bearer_token = os.environ.get('TWITTER_API_BEARER_TOKEN')\n",
    "\n",
    "# auth = tweepy.OAuth1UserHandler(consumer_key='z2y44W6tN6ASmdTP9VxKaPKQR',consumer_secret='MEdp5dgVtEcjyhM8MbkhF49P0kbVbmUQDb8HGI6q2fOl1xMm1Z',access_token='1777203173272219648-3legXuIluVdKk5TO0oxgOTX0g9ihe7',access_token_secret='h0lAYbRnY1zcFgPxj4cBsFkTutaNJhJmG4aoyDo9vS0PE')\n",
    "# api = tweepy.API(auth)\n",
    "\n",
    "# public_tweets = api.home_timeline()\n",
    "# for tweet in public_tweets:\n",
    "#     print(tweet.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# # To set your enviornment variables in your terminal run the following line:\n",
    "# # export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "# bearer_token = os.environ.get(\"TWITTER_API_BEARER_TOKEN\")\n",
    "\n",
    "\n",
    "# def create_url():\n",
    "#     tweet_fields = \"tweet.fields=lang,author_id\"\n",
    "#     # Tweet fields are adjustable.\n",
    "#     # Options include:\n",
    "#     # attachments, author_id, context_annotations,\n",
    "#     # conversation_id, created_at, entities, geo, id,\n",
    "#     # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "#     # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "#     # source, text, and withheld\n",
    "#     ids = \"ids=1278747501642657792,1255542774432063488\"\n",
    "#     # You can adjust ids to include a single Tweets.\n",
    "#     # Or you can add to up to 100 comma-separated IDs\n",
    "#     url = \"https://api.twitter.com/2/tweets?{}&{}\".format(ids, tweet_fields)\n",
    "#     return url\n",
    "\n",
    "\n",
    "# def bearer_oauth(r):\n",
    "#     \"\"\"\n",
    "#     Method required by bearer token authentication.\n",
    "#     \"\"\"\n",
    "\n",
    "#     r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "#     r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "#     return r\n",
    "\n",
    "\n",
    "# def connect_to_endpoint(url):\n",
    "#     response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "#     print(response.status_code)\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(\n",
    "#             \"Request returned an error: {} {}\".format(\n",
    "#                 response.status_code, response.text\n",
    "#             )\n",
    "#         )\n",
    "#     return response.json()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     url = create_url()\n",
    "#     json_response = connect_to_endpoint(url)\n",
    "#     print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.get_status(123428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
