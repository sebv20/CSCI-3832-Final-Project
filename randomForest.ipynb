{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "\n",
    "# random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in \n",
    "df = pd.read_csv('full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6607, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature_avg</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>text</th>\n",
       "      <th>Full_Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>is_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-03-28 14:15:51+00:00</td>\n",
       "      <td>778602262</td>\n",
       "      <td>-93.101503</td>\n",
       "      <td>44.950404</td>\n",
       "      <td>Importance of Human Intervantion</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>believer</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.082745</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>on march 29, 2008 at 8 pm, make a statement ab...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-03-28 15:36:30+00:00</td>\n",
       "      <td>778649364</td>\n",
       "      <td>-117.164720</td>\n",
       "      <td>32.715710</td>\n",
       "      <td>Global stance</td>\n",
       "      <td>0.429441</td>\n",
       "      <td>believer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>-0.132076</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>City looks at green building standards: Hoping...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2008-03-28 15:41:13+00:00</td>\n",
       "      <td>778652293</td>\n",
       "      <td>-122.355847</td>\n",
       "      <td>37.788497</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>0.092446</td>\n",
       "      <td>neutral</td>\n",
       "      <td>male</td>\n",
       "      <td>-2.324198</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>@thiskat @agray @payload Snow? In PDX? In Marc...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2008-03-28 15:56:42+00:00</td>\n",
       "      <td>778661751</td>\n",
       "      <td>-123.033121</td>\n",
       "      <td>44.939157</td>\n",
       "      <td>Ideological Positions on Global Warming</td>\n",
       "      <td>-0.337010</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.810226</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>someone alert al gore - global warming isn't w...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Salem</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2008-03-28 16:09:12+00:00</td>\n",
       "      <td>778669248</td>\n",
       "      <td>-123.364953</td>\n",
       "      <td>48.428318</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>-0.317469</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.862617</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>It's snowing in Langford and sticking!  It's t...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 created_at         id         lng        lat  \\\n",
       "0           0  2008-03-28 14:15:51+00:00  778602262  -93.101503  44.950404   \n",
       "1           1  2008-03-28 15:36:30+00:00  778649364 -117.164720  32.715710   \n",
       "2           2  2008-03-28 15:41:13+00:00  778652293 -122.355847  37.788497   \n",
       "3           3  2008-03-28 15:56:42+00:00  778661751 -123.033121  44.939157   \n",
       "4           4  2008-03-28 16:09:12+00:00  778669248 -123.364953  48.428318   \n",
       "\n",
       "                                     topic  sentiment    stance     gender  \\\n",
       "0         Importance of Human Intervantion  -0.042726  believer       male   \n",
       "1                            Global stance   0.429441  believer  undefined   \n",
       "2                         Weather Extremes   0.092446   neutral       male   \n",
       "3  Ideological Positions on Global Warming  -0.337010    denier       male   \n",
       "4                         Weather Extremes  -0.317469    denier       male   \n",
       "\n",
       "   temperature_avg  aggressiveness  \\\n",
       "0        -4.082745      aggressive   \n",
       "1        -0.132076  not aggressive   \n",
       "2        -2.324198  not aggressive   \n",
       "3        -4.810226  not aggressive   \n",
       "4        -4.862617  not aggressive   \n",
       "\n",
       "                                                text  \\\n",
       "0  on march 29, 2008 at 8 pm, make a statement ab...   \n",
       "1  City looks at green building standards: Hoping...   \n",
       "2  @thiskat @agray @payload Snow? In PDX? In Marc...   \n",
       "3  someone alert al gore - global warming isn't w...   \n",
       "4  It's snowing in Langford and sticking!  It's t...   \n",
       "\n",
       "                                        Full_Address           City  \\\n",
       "0  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...     Saint Paul   \n",
       "1  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...      San Diego   \n",
       "2  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...  San Francisco   \n",
       "3  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...          Salem   \n",
       "4  {'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...       Victoria   \n",
       "\n",
       "       Continent        Country             State is_english  \n",
       "0  North America  United States         Minnesota       True  \n",
       "1  North America  United States        California       True  \n",
       "2  North America  United States        California       True  \n",
       "3  North America  United States            Oregon      False  \n",
       "4  North America         Canada  British Columbia       True  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at df\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 1014 Negative: 1021 Neutral: 4572\n"
     ]
    }
   ],
   "source": [
    "# looking at sentiments \n",
    "pos = (df['sentiment'] > 0.5).sum()\n",
    "neg = (df['sentiment'] < -0.5).sum()\n",
    "neutral = len(df) - pos - neg\n",
    "print(f'Positive: {pos} Negative: {neg} Neutral: {neutral}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic to create classes for sentiment\n",
    "sentiment  = df[\"sentiment\"]\n",
    "\n",
    "conditions = [sentiment >= 0.5,\n",
    "              sentiment <= -0.5,\n",
    "              (sentiment >-0.5) & (sentiment <0.5)]\n",
    "choices  = [2,0,1]\n",
    "\n",
    "# create a new column in the DF based on the conditions\n",
    "df[\"label\"] = np.select(conditions, choices, \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>stance</th>\n",
       "      <th>gender</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>text</th>\n",
       "      <th>City</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Importance of Human Intervantion</td>\n",
       "      <td>believer</td>\n",
       "      <td>male</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>on march 29, 2008 at 8 pm, make a statement ab...</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global stance</td>\n",
       "      <td>believer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>City looks at green building standards: Hoping...</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>neutral</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>@thiskat @agray @payload Snow? In PDX? In Marc...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ideological Positions on Global Warming</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>someone alert al gore - global warming isn't w...</td>\n",
       "      <td>Salem</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>It's snowing in Langford and sticking!  It's t...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     topic    stance     gender  \\\n",
       "0         Importance of Human Intervantion  believer       male   \n",
       "1                            Global stance  believer  undefined   \n",
       "2                         Weather Extremes   neutral       male   \n",
       "3  Ideological Positions on Global Warming    denier       male   \n",
       "4                         Weather Extremes    denier       male   \n",
       "\n",
       "   aggressiveness                                               text  \\\n",
       "0      aggressive  on march 29, 2008 at 8 pm, make a statement ab...   \n",
       "1  not aggressive  City looks at green building standards: Hoping...   \n",
       "2  not aggressive  @thiskat @agray @payload Snow? In PDX? In Marc...   \n",
       "3  not aggressive  someone alert al gore - global warming isn't w...   \n",
       "4  not aggressive  It's snowing in Langford and sticking!  It's t...   \n",
       "\n",
       "            City      Continent        Country             State  label  \n",
       "0     Saint Paul  North America  United States         Minnesota      1  \n",
       "1      San Diego  North America  United States        California      1  \n",
       "2  San Francisco  North America  United States        California      1  \n",
       "3          Salem  North America  United States            Oregon      1  \n",
       "4       Victoria  North America         Canada  British Columbia      1  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking it out\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df = df.drop('sentiment', axis=1)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# dropping unwanted columns\n",
    "df = df.drop('id', axis=1)\n",
    "df = df.drop('lng', axis=1)\n",
    "df = df.drop('Full_Address', axis=1)\n",
    "df = df.drop('lat', axis=1)\n",
    "df = df.drop('temperature_avg', axis=1)\n",
    "df = df.drop('is_english', axis=1)\n",
    "df = df.drop('created_at', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6607, 10)\n"
     ]
    }
   ],
   "source": [
    "# loooking at shape again\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The independent features set: \n",
      "[['Importance of Human Intervantion' 'believer' 'male' 'aggressive'\n",
      "  'on march 29, 2008 at 8 pm, make a statement about climate change by turning off your lights for earth hour.'\n",
      "  'Saint Paul' 'North America' 'United States']\n",
      " ['Global stance' 'believer' 'undefined' 'not aggressive'\n",
      "  'City looks at green building standards: Hoping to do its part in reducing global warming, the city... http://tinyurl.com/37mphl'\n",
      "  'San Diego' 'North America' 'United States']\n",
      " ['Weather Extremes' 'neutral' 'male' 'not aggressive'\n",
      "  '@thiskat @agray @payload Snow? In PDX? In March?  Thanks global warming!'\n",
      "  'San Francisco' 'North America' 'United States']\n",
      " ['Ideological Positions on Global Warming' 'denier' 'male'\n",
      "  'not aggressive'\n",
      "  \"someone alert al gore - global warming isn't working!!\" 'Salem'\n",
      "  'North America' 'United States']\n",
      " ['Weather Extremes' 'denier' 'male' 'not aggressive'\n",
      "  \"It's snowing in Langford and sticking!  It's the end of March almost April.  I thought we had global warming.   ;-)\"\n",
      "  'Victoria' 'North America' 'Canada']\n",
      " ['Donald Trump versus Science' 'neutral' 'female' 'not aggressive'\n",
      "  'On Y! News 60 Minutes, Al Gore talks global warming. Video coming later today! http://60minutes.yahoo.com/'\n",
      "  'Not Found' 'North America' 'United States']\n",
      " ['Weather Extremes' 'believer' 'male' 'not aggressive'\n",
      "  'shaking snow off of my head and wondering how people can still think global warming is a conspiracy theory. http://tinyurl.com/2cabcw'\n",
      "  'Portland' 'North America' 'United States']\n",
      " ['Importance of Human Intervantion' 'denier' 'male' 'aggressive'\n",
      "  \"@sheagunther you could write a story saying Al Gore announces on Apr 1 that he's been pulling our leg on climate change\"\n",
      "  'Davis' 'North America' 'United States']\n",
      " ['Undefined / One Word Hashtags' 'neutral' 'female' 'aggressive'\n",
      "  'Colleagues in Finland joke that \"global warming\" will be great for their economy.'\n",
      "  'Atlanta' 'North America' 'United States']\n",
      " ['Ideological Positions on Global Warming' 'neutral' 'male'\n",
      "  'not aggressive' '@tjlw: Me and Al invented global warming!' 'Waterloo'\n",
      "  'North America' 'Canada']]\n",
      "The dependent variable: \n",
      "[1 1 1 1 1 1 0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into independent and dependent variables\n",
    "X = df.iloc[:,0:8].values\n",
    "y = df.iloc[:,9].values\n",
    "print('The independent features set: ')\n",
    "print(X[:10,:])\n",
    "print('The dependent variable: ')\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 21)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.90, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size = 0.5, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size = 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>stance</th>\n",
       "      <th>gender</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>text</th>\n",
       "      <th>City</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Importance of Human Intervantion</td>\n",
       "      <td>believer</td>\n",
       "      <td>male</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>on march 29, 2008 at 8 pm, make a statement ab...</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global stance</td>\n",
       "      <td>believer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>City looks at green building standards: Hoping...</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>neutral</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>@thiskat @agray @payload Snow? In PDX? In Marc...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ideological Positions on Global Warming</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>someone alert al gore - global warming isn't w...</td>\n",
       "      <td>Salem</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>It's snowing in Langford and sticking!  It's t...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     topic    stance     gender  \\\n",
       "0         Importance of Human Intervantion  believer       male   \n",
       "1                            Global stance  believer  undefined   \n",
       "2                         Weather Extremes   neutral       male   \n",
       "3  Ideological Positions on Global Warming    denier       male   \n",
       "4                         Weather Extremes    denier       male   \n",
       "\n",
       "   aggressiveness                                               text  \\\n",
       "0      aggressive  on march 29, 2008 at 8 pm, make a statement ab...   \n",
       "1  not aggressive  City looks at green building standards: Hoping...   \n",
       "2  not aggressive  @thiskat @agray @payload Snow? In PDX? In Marc...   \n",
       "3  not aggressive  someone alert al gore - global warming isn't w...   \n",
       "4  not aggressive  It's snowing in Langford and sticking!  It's t...   \n",
       "\n",
       "            City      Continent        Country             State  label  \n",
       "0     Saint Paul  North America  United States         Minnesota      1  \n",
       "1      San Diego  North America  United States        California      1  \n",
       "2  San Francisco  North America  United States        California      1  \n",
       "3          Salem  North America  United States            Oregon      1  \n",
       "4       Victoria  North America         Canada  British Columbia      1  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding \n",
    "from sklearn import preprocessing\n",
    "\n",
    "cols = ['topic', 'stance', 'gender', 'aggressiveness', 'City', 'Continent', 'Country', 'State']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in cols:\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61  6 60 27 42  1 13 30 37 20 58 57 29 44 28 26 34 54 38 25 52 19  3 18\n",
      " 16 41 48  9 59 36 15 51 21 22  8  7 46 31 11 43 35 45 55 10 32  0  5 53\n",
      " 40 33 24 17 12 23 49 47  2 50 56 14 39  4]\n"
     ]
    }
   ],
   "source": [
    "df.head\n",
    "a = df['Country'].unique()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       topic  stance  gender  aggressiveness  \\\n",
       "0         1       1       1               1   \n",
       "1         1       1       1               1   \n",
       "2         1       1       1               1   \n",
       "3         1       1       1               1   \n",
       "4         1       1       1               1   \n",
       "...     ...     ...     ...             ...   \n",
       "6602      1       1       1               1   \n",
       "6603      1       1       1               1   \n",
       "6604      1       1       1               1   \n",
       "6605      2       2       2               2   \n",
       "6606      1       1       1               1   \n",
       "\n",
       "                                                   text  City  Continent  \\\n",
       "0     on march 29, 2008 at 8 pm, make a statement ab...     1          1   \n",
       "1     City looks at green building standards: Hoping...     1          1   \n",
       "2     @thiskat @agray @payload Snow? In PDX? In Marc...     1          1   \n",
       "3     someone alert al gore - global warming isn't w...     1          1   \n",
       "4     It's snowing in Langford and sticking!  It's t...     1          1   \n",
       "...                                                 ...   ...        ...   \n",
       "6602  I just watched the new MacBook video. Unibody ...     1          1   \n",
       "6603  Help us tackle climate change by signing up at...     1          1   \n",
       "6604  Michael Meacher: The climate change disconnect...     1          1   \n",
       "6605  Saw the \"Cool Globes\" in SD's Balboa Park this...     2          2   \n",
       "6606  @agunn Is that like one of those 'the world af...     1          1   \n",
       "\n",
       "      Country  State  label                                              tfidf  \n",
       "0           1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1           1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2           1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3           1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4           1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...       ...    ...    ...                                                ...  \n",
       "6602        1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6603        1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6604        1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6605        2      2      2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6606        1      1      1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[6607 rows x 11 columns]>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import texthero as hero\n",
    "df['tfidf'] = hero.tfidf(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train = scaler.fit_transform(X_train_tfidf)\n",
    "X_test = scaler.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [42280, 5285]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-044aa32a4dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fitting Random Forest Classification to the Training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         )\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [42280, 5285]"
     ]
    }
   ],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [42280, 5285]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-6bd7e152336e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrf_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m69\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=69)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# evaluate the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         )\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [42280, 5285]"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "definitions = ['topic', 'stance', 'gender', 'aggressiveness', 'text', 'City', 'Continent', 'Country', 'State']\n",
    "y_pred = classifier.predict(X_test)\n",
    "reversefactor = dict(zip(range(3),definitions))\n",
    "y_test = np.vectorize(reversefactor.get)(y_test)\n",
    "y_pred = np.vectorize(reversefactor.get)(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
