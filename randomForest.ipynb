{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "\n",
    "# random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in \n",
    "df = pd.read_csv('full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6607, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature_avg</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>text</th>\n",
       "      <th>Full_Address</th>\n",
       "      <th>City</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>is_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2008-03-28 14:15:51+00:00</td>\n",
       "      <td>778602262</td>\n",
       "      <td>-93.101503</td>\n",
       "      <td>44.950404</td>\n",
       "      <td>Importance of Human Intervantion</td>\n",
       "      <td>-0.042726</td>\n",
       "      <td>believer</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.082745</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>on march 29, 2008 at 8 pm, make a statement ab...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-03-28 15:36:30+00:00</td>\n",
       "      <td>778649364</td>\n",
       "      <td>-117.164720</td>\n",
       "      <td>32.715710</td>\n",
       "      <td>Global stance</td>\n",
       "      <td>0.429441</td>\n",
       "      <td>believer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>-0.132076</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>City looks at green building standards: Hoping...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2008-03-28 15:41:13+00:00</td>\n",
       "      <td>778652293</td>\n",
       "      <td>-122.355847</td>\n",
       "      <td>37.788497</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>0.092446</td>\n",
       "      <td>neutral</td>\n",
       "      <td>male</td>\n",
       "      <td>-2.324198</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>@thiskat @agray @payload Snow? In PDX? In Marc...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2008-03-28 15:56:42+00:00</td>\n",
       "      <td>778661751</td>\n",
       "      <td>-123.033121</td>\n",
       "      <td>44.939157</td>\n",
       "      <td>Ideological Positions on Global Warming</td>\n",
       "      <td>-0.337010</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.810226</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>someone alert al gore - global warming isn't w...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Salem</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2008-03-28 16:09:12+00:00</td>\n",
       "      <td>778669248</td>\n",
       "      <td>-123.364953</td>\n",
       "      <td>48.428318</td>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>-0.317469</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>-4.862617</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>It's snowing in Langford and sticking!  It's t...</td>\n",
       "      <td>{'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 created_at         id         lng        lat  \\\n",
       "0           0  2008-03-28 14:15:51+00:00  778602262  -93.101503  44.950404   \n",
       "1           1  2008-03-28 15:36:30+00:00  778649364 -117.164720  32.715710   \n",
       "2           2  2008-03-28 15:41:13+00:00  778652293 -122.355847  37.788497   \n",
       "3           3  2008-03-28 15:56:42+00:00  778661751 -123.033121  44.939157   \n",
       "4           4  2008-03-28 16:09:12+00:00  778669248 -123.364953  48.428318   \n",
       "\n",
       "                                     topic  sentiment    stance     gender  \\\n",
       "0         Importance of Human Intervantion  -0.042726  believer       male   \n",
       "1                            Global stance   0.429441  believer  undefined   \n",
       "2                         Weather Extremes   0.092446   neutral       male   \n",
       "3  Ideological Positions on Global Warming  -0.337010    denier       male   \n",
       "4                         Weather Extremes  -0.317469    denier       male   \n",
       "\n",
       "   temperature_avg  aggressiveness  \\\n",
       "0        -4.082745      aggressive   \n",
       "1        -0.132076  not aggressive   \n",
       "2        -2.324198  not aggressive   \n",
       "3        -4.810226  not aggressive   \n",
       "4        -4.862617  not aggressive   \n",
       "\n",
       "                                                text  \\\n",
       "0  on march 29, 2008 at 8 pm, make a statement ab...   \n",
       "1  City looks at green building standards: Hoping...   \n",
       "2  @thiskat @agray @payload Snow? In PDX? In Marc...   \n",
       "3  someone alert al gore - global warming isn't w...   \n",
       "4  It's snowing in Langford and sticking!  It's t...   \n",
       "\n",
       "                                        Full_Address           City  \\\n",
       "0  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...     Saint Paul   \n",
       "1  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...      San Diego   \n",
       "2  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...  San Francisco   \n",
       "3  {'ISO_3166-1_alpha-2': 'US', 'ISO_3166-1_alpha...          Salem   \n",
       "4  {'ISO_3166-1_alpha-2': 'CA', 'ISO_3166-1_alpha...       Victoria   \n",
       "\n",
       "       Continent        Country             State is_english  \n",
       "0  North America  United States         Minnesota       True  \n",
       "1  North America  United States        California       True  \n",
       "2  North America  United States        California       True  \n",
       "3  North America  United States            Oregon      False  \n",
       "4  North America         Canada  British Columbia       True  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at df\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 1014 Negative: 1021 Neutral: 4572\n"
     ]
    }
   ],
   "source": [
    "# looking at sentiments \n",
    "pos = (df['sentiment'] > 0.5).sum()\n",
    "neg = (df['sentiment'] < -0.5).sum()\n",
    "neutral = len(df) - pos - neg\n",
    "print(f'Positive: {pos} Negative: {neg} Neutral: {neutral}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic to create classes for sentiment\n",
    "sentiment  = df[\"sentiment\"]\n",
    "\n",
    "conditions = [sentiment >= 0.5,\n",
    "              sentiment <= -0.5,\n",
    "              (sentiment >-0.5) & (sentiment <0.5)]\n",
    "choices  = [2,0,1]\n",
    "\n",
    "# create a new column in the DF based on the conditions\n",
    "df[\"label\"] = np.select(conditions, choices, \"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>stance</th>\n",
       "      <th>gender</th>\n",
       "      <th>aggressiveness</th>\n",
       "      <th>text</th>\n",
       "      <th>City</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Importance of Human Intervantion</td>\n",
       "      <td>believer</td>\n",
       "      <td>male</td>\n",
       "      <td>aggressive</td>\n",
       "      <td>on march 29, 2008 at 8 pm, make a statement ab...</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global stance</td>\n",
       "      <td>believer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>City looks at green building standards: Hoping...</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>neutral</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>@thiskat @agray @payload Snow? In PDX? In Marc...</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ideological Positions on Global Warming</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>someone alert al gore - global warming isn't w...</td>\n",
       "      <td>Salem</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weather Extremes</td>\n",
       "      <td>denier</td>\n",
       "      <td>male</td>\n",
       "      <td>not aggressive</td>\n",
       "      <td>It's snowing in Langford and sticking!  It's t...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>North America</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     topic    stance     gender  \\\n",
       "0         Importance of Human Intervantion  believer       male   \n",
       "1                            Global stance  believer  undefined   \n",
       "2                         Weather Extremes   neutral       male   \n",
       "3  Ideological Positions on Global Warming    denier       male   \n",
       "4                         Weather Extremes    denier       male   \n",
       "\n",
       "   aggressiveness                                               text  \\\n",
       "0      aggressive  on march 29, 2008 at 8 pm, make a statement ab...   \n",
       "1  not aggressive  City looks at green building standards: Hoping...   \n",
       "2  not aggressive  @thiskat @agray @payload Snow? In PDX? In Marc...   \n",
       "3  not aggressive  someone alert al gore - global warming isn't w...   \n",
       "4  not aggressive  It's snowing in Langford and sticking!  It's t...   \n",
       "\n",
       "            City      Continent        Country             State  label  \n",
       "0     Saint Paul  North America  United States         Minnesota      1  \n",
       "1      San Diego  North America  United States        California      1  \n",
       "2  San Francisco  North America  United States        California      1  \n",
       "3          Salem  North America  United States            Oregon      1  \n",
       "4       Victoria  North America         Canada  British Columbia      1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking it out\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df = df.drop('sentiment', axis=1)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# dropping unwanted columns\n",
    "df = df.drop('id', axis=1)\n",
    "df = df.drop('lng', axis=1)\n",
    "df = df.drop('Full_Address', axis=1)\n",
    "df = df.drop('lat', axis=1)\n",
    "df = df.drop('temperature_avg', axis=1)\n",
    "df = df.drop('is_english', axis=1)\n",
    "df = df.drop('created_at', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6607, 10)\n"
     ]
    }
   ],
   "source": [
    "# loooking at shape again\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The independent features set: \n",
      "[['Importance of Human Intervantion' 'believer' 'male' 'aggressive'\n",
      "  'on march 29, 2008 at 8 pm, make a statement about climate change by turning off your lights for earth hour.'\n",
      "  'Saint Paul' 'North America' 'United States']\n",
      " ['Global stance' 'believer' 'undefined' 'not aggressive'\n",
      "  'City looks at green building standards: Hoping to do its part in reducing global warming, the city... http://tinyurl.com/37mphl'\n",
      "  'San Diego' 'North America' 'United States']\n",
      " ['Weather Extremes' 'neutral' 'male' 'not aggressive'\n",
      "  '@thiskat @agray @payload Snow? In PDX? In March?  Thanks global warming!'\n",
      "  'San Francisco' 'North America' 'United States']\n",
      " ['Ideological Positions on Global Warming' 'denier' 'male'\n",
      "  'not aggressive'\n",
      "  \"someone alert al gore - global warming isn't working!!\" 'Salem'\n",
      "  'North America' 'United States']\n",
      " ['Weather Extremes' 'denier' 'male' 'not aggressive'\n",
      "  \"It's snowing in Langford and sticking!  It's the end of March almost April.  I thought we had global warming.   ;-)\"\n",
      "  'Victoria' 'North America' 'Canada']\n",
      " ['Donald Trump versus Science' 'neutral' 'female' 'not aggressive'\n",
      "  'On Y! News 60 Minutes, Al Gore talks global warming. Video coming later today! http://60minutes.yahoo.com/'\n",
      "  'Not Found' 'North America' 'United States']\n",
      " ['Weather Extremes' 'believer' 'male' 'not aggressive'\n",
      "  'shaking snow off of my head and wondering how people can still think global warming is a conspiracy theory. http://tinyurl.com/2cabcw'\n",
      "  'Portland' 'North America' 'United States']\n",
      " ['Importance of Human Intervantion' 'denier' 'male' 'aggressive'\n",
      "  \"@sheagunther you could write a story saying Al Gore announces on Apr 1 that he's been pulling our leg on climate change\"\n",
      "  'Davis' 'North America' 'United States']\n",
      " ['Undefined / One Word Hashtags' 'neutral' 'female' 'aggressive'\n",
      "  'Colleagues in Finland joke that \"global warming\" will be great for their economy.'\n",
      "  'Atlanta' 'North America' 'United States']\n",
      " ['Ideological Positions on Global Warming' 'neutral' 'male'\n",
      "  'not aggressive' '@tjlw: Me and Al invented global warming!' 'Waterloo'\n",
      "  'North America' 'Canada']]\n",
      "The dependent variable: \n",
      "[1 1 1 1 1 1 0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into independent and dependent variables\n",
    "X = df.iloc[:,0:8].values\n",
    "y = df.iloc[:,9].values\n",
    "print('The independent features set: ')\n",
    "print(X[:10,:])\n",
    "print('The dependent variable: ')\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 21)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.90, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size = 0.5, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size = 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 723, in fit_transform\n    self._validate_column_callables(X)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 423, in _validate_column_callables\n    columns = columns(X)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 1110, in __call__\n    raise ValueError(\nValueError: make_column_selector can only be applied to pandas dataframes\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-75659b452d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtfidf_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_cv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# For callabe scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 402, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 360, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 894, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/_set_output.py\", line 142, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 723, in fit_transform\n    self._validate_column_callables(X)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 423, in _validate_column_callables\n    columns = columns(X)\n  File \"/Applications/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\", line 1110, in __call__\n    raise ValueError(\nValueError: make_column_selector can only be applied to pandas dataframes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = \"neg_mean_absolute_percentage_error\"\n",
    "n_cv_folds = 3\n",
    "\n",
    "# pipe = make_column_transformer([\n",
    "#    (TfidfVectorizer(), ['topic', 'stance', 'gender', 'aggressiveness', 'text', 'City', 'Continent', 'Country', 'State'])\n",
    "# ]).fit(df, y_train)\n",
    "\n",
    "tfidf = make_column_transformer(\n",
    "    (\n",
    "        TfidfVectorizer(),\n",
    "        make_column_selector(dtype_include=\"category\"),\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "\n",
    "hist_tfidf = make_pipeline(\n",
    "    tfidf, HistGradientBoostingRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "tfidf_result = cross_validate(hist_tfidf, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "\n",
    "\n",
    "# vect = TfidfVectorizer()\n",
    "# pipe = make_column_transformer([\n",
    "#    (clone(vect), 'topic'),\n",
    "#    (clone(vect), 'stance'),\n",
    "#    (clone(vect), 'gender'),\n",
    "#    (clone(vect), 'aggressiveness'),\n",
    "#    (clone(vect), 'text'),\n",
    "#    (clone(vect), 'City'),\n",
    "#    (clone(vect), 'Continent'),\n",
    "#    (clone(vect), 'Country'),\n",
    "#    (clone(vect), 'State')\n",
    "#    ]).fit(df, y)\n",
    "\n",
    "# X_train_tfidf = pipe.fit_transform(X_train)\n",
    "# X_test_tfidf = pipe.fit_transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# # initialize the TF-IDF vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# # fit and transform on the training data\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # transform the test data\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)# initialize the TF-IDF vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "\n",
    "# # fit and transform on the training data\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# # transform the test data\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train = scaler.fit_transform(X_train_tfidf)\n",
    "X_test = scaler.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [42280, 5285]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-044aa32a4dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fitting Random Forest Classification to the Training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         )\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [42280, 5285]"
     ]
    }
   ],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [42280, 5285]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-6bd7e152336e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrf_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m69\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=69)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# evaluate the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         )\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [42280, 5285]"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "definitions = ['topic', 'stance', 'gender', 'aggressiveness', 'text', 'City', 'Continent', 'Country', 'State']\n",
    "y_pred = classifier.predict(X_test)\n",
    "reversefactor = dict(zip(range(3),definitions))\n",
    "y_test = np.vectorize(reversefactor.get)(y_test)\n",
    "y_pred = np.vectorize(reversefactor.get)(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
